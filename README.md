# ğŸ§  Income Prediction â€“ FastAPI, Alembic & MLflow
A full-stack machine learning pipeline to predict whether a person's income exceeds $50K/year based on census data.

This project combines modern MLOps tools and best practices to deliver a scalable, versioned, and production-ready ML system.

---

## ğŸ“¦ Project structure

```
scalable_model/               # Racine du projet ML scalable
â”‚
â”œâ”€â”€ api/                      # API FastAPI exposant le modÃ¨le pour la prÃ©diction
â”‚   â”œâ”€â”€ main.py               # Entrypoint de lâ€™API REST pour prÃ©dire les revenus
â”‚   â”œâ”€â”€ model_loader.py       # Chargement du modÃ¨le et du prÃ©processeur sauvegardÃ©s
â”‚   â”œâ”€â”€ requirements.txt      # DÃ©pendances spÃ©cifiques Ã  lâ€™API
â”‚   â”œâ”€â”€ Dockerfile            # Image Docker pour conteneuriser lâ€™API
â”‚   â””â”€â”€ training/             # Module de pipeline dâ€™entraÃ®nement
â”‚       â”œâ”€â”€ train.py          # Script d'entraÃ®nement principal
â”‚       â”œâ”€â”€ preprocess.py     # PrÃ©traitement des donnÃ©es (pipeline scikit-learn)
â”‚       â”œâ”€â”€ evaluate.py       # Ã‰valuation des performances
â”‚       â”œâ”€â”€ model.py          # Architecture du modÃ¨le (Keras)
â”‚       â””â”€â”€ __init__.py       # Initialisation du module
â”‚
â”œâ”€â”€ database/                 # Microservice de gestion de la base de donnÃ©es
â”‚   â”œâ”€â”€ main.py               # API FastAPI exposant les endpoints CRUD
â”‚   â”œâ”€â”€ db_init.py            # Script d'initialisation ou de remplissage de la base
â”‚   â”œâ”€â”€ models/               # ModÃ¨les SQLAlchemy
â”‚   â”‚   â””â”€â”€ user_model.py     # ModÃ¨le reprÃ©sentant un utilisateur
â”‚   â”œâ”€â”€ schemas/              # SchÃ©mas Pydantic pour validation dâ€™entrÃ©es
â”‚   â”‚   â””â”€â”€ user_schema.py    # SchÃ©ma dâ€™un utilisateur (UserIn, UserOut)
â”‚   â”œâ”€â”€ crud/                 # Fonctions de lecture/Ã©criture dans la base
â”‚   â”‚   â””â”€â”€ user_crud.py      # CRUD pour la table `users`
â”‚   â”œâ”€â”€ modules/              # Aides et scripts liÃ©s Ã  la DB
â”‚   â”‚   â””â”€â”€ populate_db.py    # Script pour insÃ©rer des donnÃ©es dans la DB
â”‚   â”œâ”€â”€ alembic.ini           # Configuration Alembic
â”‚   â”œâ”€â”€ alembic/              # RÃ©pertoire Alembic (migrations, env.py)
â”‚   â”‚   â”œâ”€â”€ env.py            # Configuration de lâ€™environnement Alembic
â”‚   â”‚   â””â”€â”€ versions/         # Fichiers de migration (auto-gÃ©nÃ©rÃ©s)
â”‚   â”œâ”€â”€ requirements.txt      # DÃ©pendances spÃ©cifiques au service DB
â”‚   â””â”€â”€ Dockerfile            # Image Docker pour conteneuriser le service BDD
â”‚
â”œâ”€â”€ data/                     # DonnÃ©es du projet
â”‚   â”œâ”€â”€ raw/                  # DonnÃ©es brutes initiales (ex. adult.data)
â”‚   â””â”€â”€ processed/            # DonnÃ©es transformÃ©es/filtrÃ©es
â”‚
â”œâ”€â”€ dump/                     # Fichiers gÃ©nÃ©rÃ©s liÃ©s Ã  la DB ou au dump SQL
â”‚   â””â”€â”€ generated-user.db     # Fichier SQLite contenant la table `users`
â”‚
â”œâ”€â”€ mlruns/                   # Dossier gÃ©nÃ©rÃ© par MLflow pour le suivi d'expÃ©riences
â”‚   â””â”€â”€ ...                   # Contient les artefacts, mÃ©triques, modÃ¨les versionnÃ©s
â”‚
â”œâ”€â”€ model_artifacts/          # Fichiers finaux du modÃ¨le entraÃ®nÃ©
â”‚   â”œâ”€â”€ model1.keras          # ModÃ¨le Keras exportÃ©
â”‚   â””â”€â”€ preprocessor2.pkl     # Pipeline de prÃ©traitement sauvegardÃ© (scikit-learn)
â”‚
â”œâ”€â”€ monitoring/               # Stack de monitoring (Grafana, Prometheus, Kuma)
â”‚   â”œâ”€â”€ grafana/              # Configuration de Grafana
â”‚   â”‚   â”œâ”€â”€ dashboards/       # Dashboards custom
â”‚   â”‚   â””â”€â”€ datasources/      # Sources de donnÃ©es pour Grafana
â”‚   â”œâ”€â”€ prometheus/           # Fichier de configuration Prometheus
â”‚   â”œâ”€â”€ kuma/                 # Configuration de Kuma (agent + dashboard)
â”‚   â””â”€â”€ Dockerfile            # Image Docker pour le service de monitoring
â”‚
â”œâ”€â”€ tests/                    # Tests unitaires et end-to-end
â”‚   â”œâ”€â”€ test_api.py           # Tests de lâ€™API de prÃ©diction
â”‚   â”œâ”€â”€ test_db.py            # Tests CRUD de lâ€™API de la DB
â”‚   â”œâ”€â”€ test_training.py      # Tests du pipeline d'entraÃ®nement
â”‚   â””â”€â”€ test_monitoring.py    # VÃ©rifications basiques de lâ€™intÃ©gration monitoring
â”‚
â”œâ”€â”€ assets/                   # Assets du projet (captures, visuels)
â”‚   â”œâ”€â”€ api_swagger.png
â”‚   â”œâ”€â”€ database_swagger.png
â”‚   â”œâ”€â”€ grafana.png
â”‚   â””â”€â”€ mlflow-result.png
â”‚
â”œâ”€â”€ notebook.ipynb            # Notebook dâ€™exploration (optionnel)
â”œâ”€â”€ docker-compose.yml        # Orchestration multi-conteneurs avec Docker Compose
â”œâ”€â”€ Makefile                  # Commandes CLI utiles (lint, test, format, build, etc.)
â”œâ”€â”€ requirements.txt          # DÃ©pendances globales du projet (hors services isolÃ©s)
â”œâ”€â”€ .env                      # Fichier dâ€™environnement (DB_URL, secrets, etc.)
â”œâ”€â”€ .gitignore                # Fichiers/dossiers ignorÃ©s par Git
â””â”€â”€ README.md                 # Documentation principale du projet
```
--- 

## ğŸŒ Virtual environment

**Linux**
```batch
python3 -m venv .venv
source .venv/bin/activate
```

**MacOS-Windows**
```batch
python -m venv .venv
.venv\Scripts\activate
```

---

## Alembic

**CrÃ©er une nouvelle migration**
Depuis la racine du projet :
```batch
alembic -c database/alembic.ini revision --autogenerate -m "nom de la migration"
```

**Appliquer la migration**
Depuis la racine du projet :
```batch
alembic -c database/alembic.ini upgrade head
```

---

## Database api 
Depuis la racine du projet
```batch
uvicorn database.main:app --reload
```

---

## Fastapi 
Depuis la racine du projet 
```batch
uvicorn api.main:app --host 0.0.0.0 --port 8500 --reload
```

PrÃ©dire une valeur avec curl 
```batch
curl -X POST http://localhost:8500/predict_income \
  -H "Content-Type: application/json" \
  -d '{
    "age": 39,
    "workclass": "State-gov",
    "fnlwgt": 77516,
    "education": "Bachelors",
    "education_num": 13,
    "marital_status": "Never-married",
    "occupation": "Adm-clerical",
    "relationship": "Not-in-family",
    "capital_gain": 2174,
    "capital_loss": 0,
    "hours_per_week": 40,
    "native_country": "United-States"
  }'
```
---

## Grafana 
![Grafana](/assets/grafana.png)

---

## Mlflow 
![Mlflow](/assets/mlflow-result.png)

---

## Api 
![api](/assets/api_swagger.png)

---

## Database 
![api](/assets/database_swagger.png)


## Comparaison avec CHAT GPT

MÃ©triques du nouveau modÃ¨le
| Composant     | Valeur observÃ©e       | InterprÃ©tation                                                                 |
| ------------- | --------------------- | ------------------------------------------------------------------------------ |
| **CPU Busy**  | \~4.5â€¯%               | TrÃ¨s faible charge. Usage classique d'un serveur avec trafic modÃ©rÃ©.           |
| **RAM Used**  | \~32.9â€¯% (sur 31â€¯GiB) | Utilisation mÃ©moire typique dâ€™un environnement de test/production lÃ©ger.       |
| **Disk Used** | Peu utilisÃ©           | SystÃ¨me sain, stockage stable.                                                 |
| **Network**   | < 25â€¯kb/s             | Faible trafic rÃ©seau (indique peu dâ€™entrÃ©es/sorties ou peu de connexions API). |
| **CPU Cores** | 16                    | Machine puissante, mais dans une plage standard pour des workloads modestes.   |

MÃ©triques de CHAT GPT
| Composant          | ChatGPT v4 â€“ ordre de grandeur                     | Comparaison avec votre environnement                                                |
| ------------------ | -------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **CPU Ã©quivalent** | â‰ˆ 20â€¯000+ cÅ“urs CPU pour les processus auxiliaires | Vous avez 16 cÅ“urs â†’ 1/1â€¯250e de la capacitÃ© CPU dâ€™un seul job OpenAI.              |
| **GPU**            | 10â€¯000+ GPUs A100 40/80 Go sur plusieurs semaines  | Aucun GPU actif dans votre environnement (ou dÃ©sactivÃ© via `CUDA_VISIBLE_DEVICES`). |
| **RAM totale**     | Plusieurs To (teraoctets)                          | 31 GiB â†’ \~0.003â€¯% de la RAM totale dâ€™un job dâ€™entraÃ®nement de LLM.                 |
| **Bande passante** | RÃ©seau InfiniBand 400â€¯Gb/s par nÅ“ud                | Vous Ãªtes Ã  < 1â€¯Mb/s â†’ â‰ˆ 1/400â€¯000e de la bande passante dâ€™un nÅ“ud OpenAI.          |
| **CoÃ»t Ã©nergie**   | Plusieurs millions de dollars                      | Probablement moins de quelques euros/jour pour votre serveur.                       |

Implication en matiÃ¨re de donnÃ©es personnelles

| Point de comparaison             | Votre systÃ¨me actuel                                             | ChatGPT (OpenAI, cloud-scale)                                                |
| -------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **TraÃ§abilitÃ© des utilisateurs** | Possible via IP/session/API logs                                 | ExtrÃªmement fine (logs, prompt tracking, usage analytics)                    |
| **SensibilitÃ© des donnÃ©es**      | Faible/modÃ©rÃ©e (donnÃ©es de recensement)                          | Potentiellement trÃ¨s Ã©levÃ©e (donnÃ©es entrÃ©es librement par les utilisateurs) |
| **Risque RGPD**                  | Faible Ã  modÃ©rÃ©, mais rÃ©el si les logs ne sont pas pseudonymisÃ©s | Ã‰levÃ© : stockage mondial, traitement Ã  trÃ¨s large Ã©chelle.                   |

**En bref** 

MÃªme si l'infrastructure actuelle est 1000 Ã  1â€¯000â€¯000 fois plus petite que celle nÃ©cessaire Ã  lâ€™entraÃ®nement de ChatGPT :

Les principes de conformitÃ© restent les mÃªmes : chaque donnÃ©e collectÃ©e, mÃªme indirectement via monitoring, peut constituer une donnÃ©e personnelle.

Lâ€™effet cumulatif de logs, mÃ©triques systÃ¨me et journaux API peut permettre un profilage.

En 2025, les outils permettent de voir presque tout ce que fait un utilisateur contrairement Ã  1994. Cela change radicalement la responsabilitÃ© du DPO.